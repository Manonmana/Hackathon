{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:28:45.344637Z","iopub.execute_input":"2022-11-05T17:28:45.345020Z","iopub.status.idle":"2022-11-05T17:29:36.034574Z","shell.execute_reply.started":"2022-11-05T17:28:45.344990Z","shell.execute_reply":"2022-11-05T17:29:36.032822Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py4j==0.10.9.5\n  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845513 sha256=8e87ee163d4a772434dcff49d5b547388470c87d7322f07f9b93cbd99107b0a2\n  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\n  Attempting uninstall: py4j\n    Found existing installation: py4j 0.10.9.7\n    Uninstalling py4j-0.10.9.7:\n      Successfully uninstalled py4j-0.10.9.7\nSuccessfully installed py4j-0.10.9.5 pyspark-3.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pyspark","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:29:41.947716Z","iopub.execute_input":"2022-11-05T17:29:41.948145Z","iopub.status.idle":"2022-11-05T17:29:41.953989Z","shell.execute_reply.started":"2022-11-05T17:29:41.948108Z","shell.execute_reply":"2022-11-05T17:29:41.952509Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:29:45.068680Z","iopub.execute_input":"2022-11-05T17:29:45.069120Z","iopub.status.idle":"2022-11-05T17:29:45.075283Z","shell.execute_reply.started":"2022-11-05T17:29:45.069084Z","shell.execute_reply":"2022-11-05T17:29:45.074310Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"spark = SparkSession.builder.master(\"local[2]\").appName(\"Task-1\").getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:29:49.038722Z","iopub.execute_input":"2022-11-05T17:29:49.039384Z","iopub.status.idle":"2022-11-05T17:29:54.745814Z","shell.execute_reply.started":"2022-11-05T17:29:49.039342Z","shell.execute_reply":"2022-11-05T17:29:54.744349Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"22/11/05 17:29:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"spark","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:29:57.219242Z","iopub.execute_input":"2022-11-05T17:29:57.219724Z","iopub.status.idle":"2022-11-05T17:29:58.238460Z","shell.execute_reply.started":"2022-11-05T17:29:57.219680Z","shell.execute_reply":"2022-11-05T17:29:58.237551Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<pyspark.sql.session.SparkSession at 0x7fe366658910>","text/html":"\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://9305219be002:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[2]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Task-1</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "},"metadata":{}}]},{"cell_type":"code","source":"sc = spark.sparkContext\nsc","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:30:01.954710Z","iopub.execute_input":"2022-11-05T17:30:01.955171Z","iopub.status.idle":"2022-11-05T17:30:01.964690Z","shell.execute_reply.started":"2022-11-05T17:30:01.955131Z","shell.execute_reply":"2022-11-05T17:30:01.962325Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<SparkContext master=local[2] appName=Task-1>","text/html":"\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://9305219be002:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[2]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Task-1</code></dd>\n            </dl>\n        </div>\n        "},"metadata":{}}]},{"cell_type":"code","source":"sqlContext = SQLContext(spark.sparkContext)\nsqlContext","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:30:06.110167Z","iopub.execute_input":"2022-11-05T17:30:06.110602Z","iopub.status.idle":"2022-11-05T17:30:06.134298Z","shell.execute_reply.started":"2022-11-05T17:30:06.110567Z","shell.execute_reply":"2022-11-05T17:30:06.133001Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pyspark/sql/context.py:114: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  FutureWarning,\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<pyspark.sql.context.SQLContext at 0x7fe3665fb850>"},"metadata":{}}]},{"cell_type":"code","source":"df=spark.read.csv(\"../input/task-1\",header=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:30:15.200249Z","iopub.execute_input":"2022-11-05T17:30:15.200702Z","iopub.status.idle":"2022-11-05T17:30:20.453670Z","shell.execute_reply.started":"2022-11-05T17:30:15.200669Z","shell.execute_reply":"2022-11-05T17:30:20.452632Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:30:23.537658Z","iopub.execute_input":"2022-11-05T17:30:23.538084Z","iopub.status.idle":"2022-11-05T17:30:23.881557Z","shell.execute_reply.started":"2022-11-05T17:30:23.538052Z","shell.execute_reply":"2022-11-05T17:30:23.880120Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"+--------------+-------------+--------------------+-----+--------------------+--------------+-----------+\n|RP State Plate|Ticket number|          Issue Date|Color|            Location|Violation code|Fine amount|\n+--------------+-------------+--------------------+-----+--------------------+--------------+-----------+\n|            CA|   1108319494|2015-12-22T00:00:...|   WH|    12355 BURBANK BL|         5200A|       25.0|\n|            CA|   1110265251|2015-12-16T00:00:...|   WH|ORION AVE/SHERMAN WY|         5204A|       25.0|\n|            CA|   1111967194|2015-12-22T00:00:...|   YE|   700 WORLD WAY L/L|           007|       68.0|\n|            CA|   1111967205|2015-12-28T00:00:...|   WH|   500 WORLD WAY U/L|          8939|       58.0|\n|            CA|   1112069534|2015-12-23T00:00:...|   WH|       700 WORLD WAY|          8939|       58.0|\n|            CA|   1112094410|2015-12-15T00:00:...|   GY|   9019 S AIRPORT BL|         80714|       68.0|\n|            CA|   1112096263|2015-12-26T00:00:...|   SI|    101 WORLD WAY 2B|        4000A1|       50.0|\n|            CA|   1112716640|2015-12-28T00:00:...|   RE|     2210 W COURT ST|        8069BS|       73.0|\n|            CA|   1112716651|2015-12-28T00:00:...|   WH|  W/S 100 N ROSELAKE|        8069BS|       73.0|\n|            OR|   1112717491|2015-12-28T00:00:...|   RE|   1837 N WHITLEY AV|        8069BS|       73.0|\n|            CA|   1112718073|2015-12-28T00:00:...|   BK|  A/F 4454 W AVOCADO|           098|       25.0|\n|            CA|   1113011513|2015-12-12T00:00:...|   GR|       50TH W/O MAIN|        4000A1|       50.0|\n|            CA|   1113011841|2015-12-11T00:00:...|   SI|             62/WALL|        4000A1|       50.0|\n|            CA|   1113879793|2015-12-24T00:00:...|   BK|     2370 S WESTWOOD|        8069AP|       93.0|\n|            ND|   1113964773|2015-12-24T00:00:...|   MA|    3760 WATSEKA AVE|        8069BS|       73.0|\n|            CA|   1113964806|2015-12-24T00:00:...|   GY|   9837 TABOR STREET|        8069BS|       73.0|\n|            CA|   1113964832|2015-12-24T00:00:...|   WH|       3614 FARIS DR|        8069BS|       73.0|\n|            CA|   1113965123|2015-12-24T00:00:...|   MA| 3233 SEPULVEDA BLVD|        8069BS|       73.0|\n|            CA|   1113965274|2015-12-24T00:00:...| null|    3447 VETERAN AVE|        8069BS|       73.0|\n|            CA|   1113965355|2015-12-27T00:00:...|   WH|  9165 ALCOTT STREET|         8058L|       68.0|\n+--------------+-------------+--------------------+-----+--------------------+--------------+-----------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"df2=df.withColumnRenamed(\"Fine Amount\",\"Fine\")","metadata":{"execution":{"iopub.status.busy":"2022-11-05T16:34:07.696696Z","iopub.execute_input":"2022-11-05T16:34:07.697168Z","iopub.status.idle":"2022-11-05T16:34:07.718366Z","shell.execute_reply.started":"2022-11-05T16:34:07.697128Z","shell.execute_reply":"2022-11-05T16:34:07.716978Z"}}},{"cell_type":"code","source":"df2=df.withColumnRenamed(\"Fine Amount\",\"Fine\")","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:30:29.882125Z","iopub.execute_input":"2022-11-05T17:30:29.882573Z","iopub.status.idle":"2022-11-05T17:30:29.899506Z","shell.execute_reply.started":"2022-11-05T17:30:29.882534Z","shell.execute_reply":"2022-11-05T17:30:29.898097Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df3 = df2.withColumn(\"Fine\", df2[\"Fine\"].cast(\"double\"))","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:32:12.880854Z","iopub.execute_input":"2022-11-05T17:32:12.881249Z","iopub.status.idle":"2022-11-05T17:32:12.900568Z","shell.execute_reply.started":"2022-11-05T17:32:12.881217Z","shell.execute_reply":"2022-11-05T17:32:12.899280Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"df3.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:31:42.945290Z","iopub.execute_input":"2022-11-05T17:31:42.945718Z","iopub.status.idle":"2022-11-05T17:31:42.954921Z","shell.execute_reply.started":"2022-11-05T17:31:42.945684Z","shell.execute_reply":"2022-11-05T17:31:42.953701Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[('RP State Plate', 'string'),\n ('Ticket number', 'string'),\n ('Issue Date', 'string'),\n ('Color', 'string'),\n ('Location', 'string'),\n ('Violation code', 'string'),\n ('Fine', 'int')]"},"metadata":{}}]},{"cell_type":"code","source":"df4=df3.groupBy('RP State Plate').avg('Fine')","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:37:33.103299Z","iopub.execute_input":"2022-11-05T17:37:33.104144Z","iopub.status.idle":"2022-11-05T17:37:33.132818Z","shell.execute_reply.started":"2022-11-05T17:37:33.104096Z","shell.execute_reply":"2022-11-05T17:37:33.131589Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"df4.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-05T17:37:35.376172Z","iopub.execute_input":"2022-11-05T17:37:35.376626Z","iopub.status.idle":"2022-11-05T17:37:37.674905Z","shell.execute_reply.started":"2022-11-05T17:37:35.376588Z","shell.execute_reply":"2022-11-05T17:37:37.673646Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"[Stage 3:>                                                          (0 + 2) / 2]\r","output_type":"stream"},{"name":"stdout","text":"+--------------+-----------------+\n|RP State Plate|        avg(Fine)|\n+--------------+-----------------+\n|            SC|73.14909090909092|\n|            AZ|76.12789892696435|\n|            NS|           71.125|\n|            LA|74.13555555555556|\n|            MN|          73.1632|\n|            NJ|72.62927981109799|\n|            MX|70.70103092783505|\n|            DC|70.34126984126983|\n|            CN|             74.0|\n|            OR|74.42333709131906|\n|            NW|             73.0|\n|            VA|72.60915492957747|\n|            CZ|             70.5|\n|          null|71.47619047619048|\n|            RI|             72.7|\n|            WY|73.07446808510639|\n|            KY|74.14761904761905|\n|            BC|71.38709677419355|\n|            NH|73.12396694214875|\n|            MI|74.17245119305856|\n+--------------+-----------------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-11-05T16:17:24.985722Z","iopub.execute_input":"2022-11-05T16:17:24.986158Z","iopub.status.idle":"2022-11-05T16:17:25.004557Z","shell.execute_reply.started":"2022-11-05T16:17:24.986123Z","shell.execute_reply":"2022-11-05T16:17:25.003332Z"},"trusted":true},"execution_count":55,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2055071270.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fine\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'double'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1989\u001b[0;31m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m             )\n\u001b[1;32m   1991\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Fine'"],"ename":"AttributeError","evalue":"'DataFrame' object has no attribute 'Fine'","output_type":"error"}]}]}